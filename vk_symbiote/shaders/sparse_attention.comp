#version 450
#extension GL_ARB_separate_shader_objects : enable

// SPARSE ATTENTION SHADER
// Supports Longformer-style sliding window + global attention
// Complexity: O(seq_len × window_size) instead of O(seq_len²)
// Enables 200K+ token context windows

layout(local_size_x = 256) in;

// Input/output activation buffers
layout(binding = 0, std430) readonly buffer InputBuf { float input_data[]; };
layout(binding = 1, std430) writeonly buffer OutputBuf { float output_data[]; };

// Weight buffers
layout(binding = 2, std430) readonly buffer QueryWeight { float q_weight[]; };
layout(binding = 3, std430) readonly buffer KeyWeight { float k_weight[]; };
layout(binding = 4, std430) readonly buffer ValueWeight { float v_weight[]; };
layout(binding = 5, std430) readonly buffer OutputWeight { float o_weight[]; };

// KV Cache buffers
layout(binding = 6, std430) readonly buffer KeyCache { float key_cache[]; };
layout(binding = 7, std430) readonly buffer ValueCache { float value_cache[]; };

// Global attention indices (for tokens that attend to all positions)
layout(binding = 8, std430) readonly buffer GlobalAttentionMask { uint global_attn_mask[]; };

layout(push_constant) uniform PC {
    uint32_t seq_len;
    uint32_t head_dim;
    uint32_t num_heads;
    uint32_t num_kv_heads;
    uint32_t window_size;      // Sliding window size (e.g., 512)
    uint32_t global_tokens;    // Number of global tokens
    uint32_t max_seq_len;      // Maximum sequence length
    float scale;
    bool use_cache;
    bool use_sparse;           // Enable sparse attention patterns
} pc;

// Shared memory for Q values and intermediate results
shared float shared_q[256];
shared float shared_scores[512];  // Window size for softmax

// Sparse attention mask patterns
const uint SPARSE_LOCAL_LEFT  = 0;   // Attend to left window
const uint SPARSE_LOCAL_RIGHT = 1;   // Attend to right window
const uint SPARSE_GLOBAL      = 2;   // Global attention (attend to all)
const uint SPARSE_DILATED     = 3;   // Dilated attention (every Nth token)

// Compute attention score with a specific key position
float compute_attention_score(uint32_t head, uint32_t kv_head, uint32_t pos_q, uint32_t pos_k, 
                              uint32_t head_size, uint32_t num_heads_per_kv) {
    float score = 0.0;
    
    // Load Q from shared memory
    for (uint32_t d = 0; d < head_size; ++d) {
        float q_val = shared_q[d];
        
        // Load K from cache
        uint32_t k_idx;
        if (pc.use_cache) {
            k_idx = ((kv_head * pc.max_seq_len + pos_k) * head_size + d);
        } else {
            k_idx = ((kv_head * pc.seq_len + pos_k) * head_size + d);
        }
        float k_val = key_cache[k_idx];
        
        score += q_val * k_val;
    }
    
    return score * pc.scale;
}

// Get attention window for a specific position
// Returns: [start_idx, end_idx) of tokens to attend to
void get_attention_window(uint32_t pos, out uint32_t start, out uint32_t end) {
    if (pc.use_sparse) {
        // Sparse pattern: sliding window + global tokens
        
        // Always attend to global tokens (at beginning of sequence)
        uint32_t global_end = pc.global_tokens;
        
        // Sliding window: attend to window_size tokens before and after
        uint32_t window_start = (pos > pc.window_size) ? (pos - pc.window_size) : 0;
        uint32_t window_end = min(pos + pc.window_size + 1, pc.seq_len);
        
        // Combine: global tokens + sliding window
        if (pos < global_end) {
            // Global tokens attend to everything (or all previous tokens)
            start = 0;
            end = pc.use_cache ? pc.seq_len : (pos + 1);
        } else {
            // Regular tokens: global tokens + local window
            start = min(global_end, window_start);
            end = window_end;
        }
    } else {
        // Full attention (for comparison/testing)
        start = pc.use_cache ? 0 : 0;
        end = pc.use_cache ? pc.seq_len : (pos + 1);
    }
}

// Ring attention: divide sequence into blocks, compute block-wise attention
// This reduces memory bandwidth by processing blocks sequentially
void compute_ring_attention(uint32_t head, uint32_t kv_head, uint32_t pos, 
                           uint32_t head_size, inout float output[256]) {
    // Block size for ring attention (e.g., 1024 tokens per block)
    const uint32_t BLOCK_SIZE = 1024;
    
    uint32_t num_blocks = (pc.seq_len + BLOCK_SIZE - 1) / BLOCK_SIZE;
    uint32_t current_block = pos / BLOCK_SIZE;
    
    // Current token only needs to attend to:
    // 1. All tokens in its own block (local attention)
    // 2. Representative tokens from previous blocks (compressed)
    
    // For simplicity, we use sliding window within block + global summary
    // Real ring attention would use block-wise KV compression
    
    uint32_t block_start = current_block * BLOCK_SIZE;
    uint32_t block_end = min(block_start + BLOCK_SIZE, pc.seq_len);
    
    // Get attention window
    uint32_t attend_start, attend_end;
    get_attention_window(pos, attend_start, attend_end);
    
    // Clamp to current block + previous block summaries
    attend_start = max(attend_start, block_start > pc.window_size ? block_start - pc.window_size : 0);
    attend_end = min(attend_end, block_end);
    
    // Compute attention over the window
    float max_score = -1e20;
    uint32_t window_len = attend_end - attend_start;
    
    // First pass: compute max score
    for (uint32_t k_pos = attend_start; k_pos < attend_end; ++k_pos) {
        float score = compute_attention_score(head, kv_head, pos, k_pos, head_size, 
                                              pc.num_heads / pc.num_kv_heads);
        
        // Store in shared memory for softmax
        uint32_t idx = k_pos - attend_start;
        if (idx < 512) {
            shared_scores[idx] = score;
        }
        
        max_score = max(max_score, score);
    }
    
    barrier();
    
    // Second pass: compute softmax sum
    float sum = 0.0;
    for (uint32_t k_pos = attend_start; k_pos < attend_end; ++k_pos) {
        uint32_t idx = k_pos - attend_start;
        float score = (idx < 512) ? shared_scores[idx] : 0.0;
        sum += exp(score - max_score);
    }
    
    // Third pass: weighted sum of values
    for (uint32_t d = 0; d < head_size; ++d) {
        output[d] = 0.0;
    }
    
    for (uint32_t k_pos = attend_start; k_pos < attend_end; ++k_pos) {
        uint32_t idx = k_pos - attend_start;
        float score = (idx < 512) ? shared_scores[idx] : 0.0;
        float attn = exp(score - max_score) / sum;
        
        // Load V and accumulate
        for (uint32_t d = 0; d < head_size; ++d) {
            uint32_t v_idx;
            if (pc.use_cache) {
                v_idx = ((kv_head * pc.max_seq_len + k_pos) * head_size + d);
            } else {
                v_idx = ((kv_head * pc.seq_len + k_pos) * head_size + d);
            }
            float v_val = value_cache[v_idx];
            output[d] += attn * v_val;
        }
    }
}

void main() {
    uint32_t gid = gl_GlobalInvocationID.x;
    uint32_t head = gid / pc.seq_len;
    uint32_t pos = gid % pc.seq_len;
    
    if (head >= pc.num_heads || pos >= pc.seq_len) return;
    
    uint32_t head_size = pc.head_dim;
    uint32_t kv_head = head * pc.num_kv_heads / pc.num_heads;
    uint32_t num_heads_per_kv = pc.num_heads / pc.num_kv_heads;
    
    // Load Q for this position
    // TODO: Compute Q from input instead of loading
    for (uint32_t d = gl_LocalInvocationID.x; d < head_size; d += gl_WorkGroupSize.x) {
        // Placeholder: load from input or compute Q projection
        uint32_t q_idx = (head * pc.seq_len + pos) * head_size + d;
        shared_q[d] = q_weight[q_idx]; // Simplified
    }
    barrier();
    
    // Compute sparse attention
    float attn_output[256];
    compute_ring_attention(head, kv_head, pos, head_size, attn_output);
    
    // Apply output projection
    barrier();
    
    for (uint32_t d = gl_LocalInvocationID.x; d < head_size; d += gl_WorkGroupSize.x) {
        float out_val = 0.0;
        for (uint32_t i = 0; i < head_size; ++i) {
            out_val += attn_output[i] * o_weight[i * head_size + d];
        }
        
        uint32_t out_idx = (head * pc.seq_len + pos) * head_size + d;
        output_data[out_idx] = out_val;
    }
}
